{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "893433fc-1ff8-4053-9109-142834fa6830",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Configuración e Importación de Librerías\n",
    "\n",
    "En esta sección inicializamos Spark e importamos las librerías necesarias que se utilizarán a lo largo del proyecto.\n",
    "\n",
    "## Inicialización de Spark\n",
    "Comenzamos importando e inicializando `findspark` para asegurarnos de que PySpark esté accesible e integrado dentro del notebook. Esto es especialmente útil cuando trabajamos con Spark fuera de su entorno predeterminado (por ejemplo, Databricks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07d4627-e56f-4dc7-91ee-2fe084af126c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e78ccf-f76b-45e0-bf9f-3832c4cb0ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark y Manipulación de Datos\n",
    "El núcleo del proyecto involucra el uso de PySpark para el procesamiento de datos y la ejecución de tareas de machine learning. Importamos pyspark y pandas, lo que nos permitirá manejar dataframes e interactuar con el motor de cómputo distribuido de Spark. El objeto SparkSession es el punto de entrada para interactuar con Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9e9ffc-0587-4099-b592-86c74168f15a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "995e146d-01ff-4e66-8c2a-06a98aa5d0c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Librerías de Machine Learning\n",
    "Importamos varias librerías de machine learning de PySpark que se utilizarán para crear modelos, como LogisticRegression, DecisionTreeClassifier y RandomForestClassifier. Además, importamos Pipeline de PySpark para estructurar nuestro flujo de trabajo de machine learning y MulticlassClassificationEvaluator para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8c395d-5903-4e1d-a782-7c406f8ffffd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Explicación de los Componentes Importados:\n",
    "- VectorAssembler: Combina múltiples columnas en un solo vector de características, lo cual es esencial para los modelos de Spark ML.\n",
    "- PCA (Análisis de Componentes Principales): Utilizado para la reducción de dimensionalidad, mejorando el rendimiento y la eficiencia del modelo.\n",
    "- LogisticRegression, DecisionTreeClassifier, RandomForestClassifier: Algoritmos de clasificación que se emplearán para crear nuestros modelos de clasificación de vinos.\n",
    "- Pipeline: Una abstracción de alto nivel en PySpark ML que permite encadenar varios pasos (por ejemplo, transformaciones de datos y modelado) en un flujo de trabajo unificado.\n",
    "- MulticlassClassificationEvaluator, BinaryClassificationEvaluator: Evalúan el rendimiento de los modelos de clasificación. Aunque las métricas de evaluación binarias son más relevantes para problemas binarios, se adaptarán para problemas de clasificación multiclase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d78028e-3c16-437a-935f-7f055638d8fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf4aa78a-cd4e-4c44-8a4e-9f14239bc206",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Perfilado de Datos\n",
    "Utilizamos ydata_profiling para realizar un análisis y exploración inicial de los datos. Esta librería genera reportes completos sobre los datos, incluyendo correlaciones, valores faltantes y distribuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f54d52a-4c7b-4cc3-8213-d1430482dac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1811cb79-af5d-41ab-8d4d-b891523faf0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Validación Cruzada y Ajuste de Hiperparámetros\n",
    "Para optimizar nuestro modelo, usamos CrossValidator y ParamGridBuilder para realizar validación cruzada y ajustar los hiperparámetros. Esto asegura que nuestro modelo generalice bien con datos no vistos y previene el sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f635a1b6-9393-413e-8819-b04b7a25eaee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b6ddb7e-bc12-4f0d-acce-8be0d50d8f60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualización de Datos\n",
    "Utilizamos seaborn y matplotlib para generar visualizaciones que nos ayuden a explorar los datos e interpretar los resultados de nuestros modelos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1f67a6-4c2f-44e2-b316-cdae9b87ca89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c34c8ce-7228-46bc-9669-5da9fc9ecea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# set the experiment id\n",
    "mlflow.set_experiment(experiment_name=\"/Users/<username>/mlflow\")\n",
    "\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e33ee2b9-71a7-4ae5-9acf-9db395d5ad01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Carga de Datos y Análisis Inicial con Pandas\n",
    "\n",
    "En esta sección, se carga el conjunto de datos desde una URL utilizando **Pandas** y se realiza un análisis inicial de los datos con la librería **ydata_profiling** para comprender mejor las características y la estructura del conjunto de datos.\n",
    "\n",
    "## Lectura del Conjunto de Datos desde una URL\n",
    "Utilizamos la función `read_csv` de **Pandas** para leer el archivo CSV que contiene los datos del conjunto de vinos. El archivo se encuentra alojado en el repositorio de la **UCI Machine Learning Repository**.\n",
    "\n",
    "### URL de los Datos:\n",
    "- URL: [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)\n",
    "\n",
    "El conjunto de datos incluye información química y visual sobre diferentes muestras de vino, clasificadas en tres tipos (clases). Para identificar claramente las columnas, las etiquetamos según la descripción proporcionada en el archivo de nombres asociado.\n",
    "\n",
    "### Etiquetas de las columnas:\n",
    "- **Class**: Clase de vino (1, 2 o 3)\n",
    "- **Alcohol**: Contenido de alcohol\n",
    "- **Malic_acid**: Ácido málico\n",
    "- **Ash**: Ceniza\n",
    "- **Alcalinity_of_ash**: Alcalinidad de la ceniza\n",
    "- **Magnesium**: Magnesio\n",
    "- **Total_phenols**: Fenoles totales\n",
    "- **Flavanoids**: Flavonoides\n",
    "- **Nonflavanoid_phenols**: Fenoles no flavonoides\n",
    "- **Proanthocyanins**: Proantocianidinas\n",
    "- **Color_intensity**: Intensidad del color\n",
    "- **Hue**: Tono\n",
    "- **OD280_OD315_of_diluted_wines**: OD280/OD315 de vinos diluidos (proporción entre las absorbancias a 280 y 315 nm)\n",
    "- **Proline**: Prolina (aminoácido)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cd6b21-9886-42db-ad59-1c0bc979b233",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el archivo CSV desde la URL usando pandas\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "columns = ['Class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', \n",
    "           'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', \n",
    "           'Color_intensity', 'Hue', 'OD280_OD315_of_diluted_wines', 'Proline']\n",
    "\n",
    "df_pandas = pd.read_csv(url, names=columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e690269f-0955-429c-b549-6ae83c79eb4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pfr = ProfileReport(df_pandas)\n",
    "pfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86960715-9457-4bfb-89f4-363b200c7d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Identificación de valores atípicos utilizando el método IQR\n",
    "\n",
    "En este apartado, se implementa el **método del Rango Intercuartílico (IQR)** para identificar posibles valores atípicos en nuestro conjunto de datos.\n",
    "\n",
    "El método IQR es una técnica ampliamente utilizada para detectar valores que se encuentran significativamente alejados del rango intercuartílico, definido como la diferencia entre el tercer cuartil (Q3) y el primer cuartil (Q1) de cada columna. Los valores que se encuentran por debajo de **Q1 - 1.5 * IQR** o por encima de **Q3 + 1.5 * IQR** son considerados valores atípicos.\n",
    "\n",
    "El proceso consta de los siguientes pasos:\n",
    "\n",
    "1. **Cálculo de los cuartiles Q1 y Q3:**\n",
    "   Se calculan los cuartiles primero y tercero de cada columna numérica del DataFrame para definir el rango intercuartílico.\n",
    "\n",
    "2. **Cálculo del IQR:**\n",
    "   El **IQR** se obtiene como la diferencia entre los cuartiles Q3 y Q1.\n",
    "\n",
    "3. **Detección de valores atípicos:**\n",
    "   Se identifican los valores que están por debajo de **Q1 - 1.5 * IQR** o por encima de **Q3 + 1.5 * IQR** y se cuentan cuántos valores atípicos existen en cada columna.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "368b7b70-0ae4-4233-9a28-3ee431fb01cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identificación de valores atípicos utilizando el método IQR\n",
    "Q1 = df_pandas.quantile(0.25)\n",
    "Q3 = df_pandas.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_pandas < (Q1 - 1.5 * IQR)) | (df_pandas > (Q3 + 1.5 * IQR))).sum()\n",
    "print(\"Valores atípicos por columna:\\n\", outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721229e8-86ac-4f84-8b90-078bcab29911",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imputación de valores atípicos utilizando la mediana\n",
    "\n",
    "Después de identificar los valores atípicos, es crucial tratarlos adecuadamente para mejorar el rendimiento de los modelos predictivos. En este caso, hemos decidido **imputar** los valores atípicos con la **mediana** de cada columna.\n",
    "\n",
    "La elección de la mediana es adecuada para este tipo de imputación, ya que, a diferencia de la media, no se ve influenciada por los valores extremos, lo que permite que los datos se mantengan más representativos y equilibrados.\n",
    "\n",
    "#### Pasos del proceso:\n",
    "\n",
    "1. **Cálculo de la mediana**:\n",
    "   Para cada columna del conjunto de datos, se calcula la mediana de los valores. La mediana es el valor que divide a los datos en dos partes iguales y es resistente a los valores extremos.\n",
    "\n",
    "2. **Imputación de valores atípicos**:\n",
    "   Se reemplazan los valores que se encuentran fuera del rango definido por el método IQR con la mediana de la columna correspondiente.\n",
    "\n",
    "3. **Actualización del DataFrame**:\n",
    "   El DataFrame es actualizado con los nuevos valores imputados en lugar de los valores atípicos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e84a944-0f48-4b60-83d2-169c65fc7152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imputar los valores atípicos con la mediana\n",
    "for column in df_pandas.columns:\n",
    "    median = df_pandas[column].median()\n",
    "    df_pandas[column] = df_pandas[column].mask((df_pandas[column] < (Q1[column] - 1.5 * IQR[column])) | (df_pandas[column] > (Q3[column] + 1.5 * IQR[column])), median)\n",
    "\n",
    "print(\"Datos después de imputar valores atípicos:\\n\", df_pandas.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75a9f1e-6c4c-4640-8761-73838086760e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creación de la sesión de Spark\n",
    "\n",
    "Una de las primeras tareas en cualquier flujo de trabajo que utiliza **Apache Spark** es la creación de una sesión de Spark. Esta sesión es el punto de entrada principal para interactuar con los clústeres de Spark y realizar tareas de procesamiento de datos a gran escala.\n",
    "\n",
    "En este proyecto, se utiliza **Spark** para procesar, transformar y analizar el conjunto de datos de clasificación de vinos, lo que permite aprovechar su capacidad para el manejo eficiente de grandes volúmenes de datos.\n",
    "\n",
    "#### Descripción del proceso:\n",
    "\n",
    "1. **Inicialización de SparkSession**:\n",
    "   - La sesión de Spark se inicializa mediante el constructor `SparkSession.builder()`. \n",
    "   - Se define un nombre de aplicación, en este caso, `\"WineClassification\"`, para identificar esta sesión en el clúster de Spark.\n",
    "   - La función `getOrCreate()` asegura que se cree una nueva sesión de Spark si no existe una, o reutiliza una existente si ya está en ejecución.\n",
    "\n",
    "2. **AppName**:\n",
    "   - Es una buena práctica dar un nombre a tu aplicación, lo cual facilita el seguimiento de las sesiones en un clúster, especialmente cuando hay múltiples aplicaciones ejecutándose de manera concurrente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58aa63f-8958-402a-b1f8-e755feedcacd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear la sesión de Spark\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WineClassification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20716eed-7932-4254-aa6e-13f76e6d820b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Conversión de DataFrame de pandas a Spark DataFrame\n",
    "\n",
    "En este paso, convertimos el **DataFrame de pandas** a un **Spark DataFrame**. Dado que Spark trabaja con grandes volúmenes de datos y distribuye las operaciones en múltiples nodos, es necesario transformar los datos desde pandas (que funciona en un solo nodo) a un formato distribuido compatible con Spark.\n",
    "\n",
    "#### Descripción del proceso:\n",
    "\n",
    "1. **Conversión de DataFrame de pandas a Spark DataFrame**:\n",
    "   - El método `createDataFrame()` de la sesión de Spark se utiliza para convertir el DataFrame que ya hemos cargado y procesado en pandas.\n",
    "   - Esta conversión es necesaria para que Spark pueda distribuir los datos y realizar operaciones a gran escala, como la clasificación, regresión y otras tareas de machine learning.\n",
    "\n",
    "2. **Mostrar los primeros registros**:\n",
    "   - Después de la conversión, utilizamos el método `show()` para visualizar las primeras filas del DataFrame en Spark.\n",
    "   - En este caso, se muestran las primeras 5 filas para verificar que la conversión se realizó correctamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5068cce4-631f-4f65-92ae-e4fb92990243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertir DataFrame de pandas a Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff82490-51d5-4498-8047-675370062b2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creación del VectorAssembler para transformación de características\n",
    "\n",
    "En esta sección, utilizamos **VectorAssembler**, una herramienta fundamental en **PySpark MLlib** para transformar múltiples columnas de características en una sola columna llamada `features`. Esta transformación es necesaria porque muchos algoritmos de machine learning en Spark requieren que los datos de entrada estén en forma de un solo vector.\n",
    "\n",
    "#### Descripción del proceso:\n",
    "\n",
    "1. **Selección de las columnas de características**:\n",
    "   - Las características que se van a utilizar en el modelo se encuentran en todas las columnas excepto la columna `Class`, ya que esta última es nuestra variable objetivo.\n",
    "   - Utilizamos `df_spark.columns[1:]` para seleccionar todas las columnas desde la segunda en adelante, excluyendo así la primera columna (`Class`), que corresponde a las etiquetas o clases.\n",
    "\n",
    "2. **VectorAssembler**:\n",
    "   - Este transformador toma una lista de columnas de entrada y las combina en una sola columna vectorial. En nuestro caso, queremos que todas las características queden condensadas en una nueva columna llamada `features`, que será la entrada para los algoritmos de clasificación.\n",
    "   - Este paso es crucial porque Spark MLlib espera que las características de entrada estén en formato vectorial (denso o disperso) para la mayoría de los algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1155aa1d-8612-47ac-9b16-f24468b59dca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_columns = df_spark.columns[1:]  # Excluir la columna 'Class'\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "934b7ef7-e330-4197-89e0-de1c061ba77a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### División de los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "Una de las prácticas esenciales en la construcción de modelos de machine learning es dividir el conjunto de datos en dos partes: un conjunto de entrenamiento y un conjunto de prueba. Esta separación es importante para evaluar la capacidad predictiva del modelo en datos no vistos, lo que evita el sobreajuste y proporciona una mejor estimación del rendimiento en el mundo real.\n",
    "\n",
    "En este paso, utilizamos el método **randomSplit** de PySpark para realizar esta división.\n",
    "\n",
    "#### Descripción del proceso:\n",
    "\n",
    "1. **randomSplit**:\n",
    "   - Este método permite dividir un **DataFrame** en varios subconjuntos de forma aleatoria, basándose en los pesos que le proporcionemos.\n",
    "   - En nuestro caso, queremos dividir los datos en un 80% para entrenamiento y un 20% para prueba.\n",
    "   - Además, utilizamos una semilla fija (**seed**) para garantizar que la división sea reproducible. Esto es útil cuando quieres obtener los mismos subconjuntos al ejecutar el código varias veces.\n",
    "\n",
    "2. **Conjunto de entrenamiento**:\n",
    "   - El conjunto de entrenamiento se utiliza para ajustar (entrenar) el modelo, es decir, para enseñarle a reconocer patrones en los datos.\n",
    "\n",
    "3. **Conjunto de prueba**:\n",
    "   - El conjunto de prueba se reserva para evaluar el rendimiento del modelo después del entrenamiento. Este conjunto no se utiliza durante el ajuste del modelo, lo que permite evaluar de manera imparcial el desempeño del modelo en datos no vistos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946c8892-01ff-4431-8e40-5b67cfbb435a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd970b40-e422-4063-a3e9-1b8537865dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creación y entrenamiento de varios modelos de clasificación utilizando pipelines\n",
    "\n",
    "En este paso, estamos construyendo tres modelos de clasificación diferentes para abordar el problema de clasificación multiclase en el dataset de vinos. Utilizamos **pipelines** de PySpark para organizar el flujo de trabajo, asegurando que el proceso de transformación y entrenamiento sea eficiente y reproducible.\n",
    "\n",
    "#### Modelos utilizados:\n",
    "\n",
    "1. **Regresión Logística (Logistic Regression)**:\n",
    "   - Este modelo es uno de los métodos más comunes para la clasificación. Aunque suele utilizarse para problemas binarios, también puede adaptarse para problemas de clasificación multiclase utilizando una estrategia de softmax.\n",
    "   - **Ventajas**: Simplicidad y eficiencia para conjuntos de datos linealmente separables.\n",
    "\n",
    "2. **Árbol de Decisión (Decision Tree)**:\n",
    "   - Los árboles de decisión son modelos interpretables que dividen los datos en subgrupos basados en características, realizando decisiones simples en cada paso.\n",
    "   - **Ventajas**: Su facilidad para manejar relaciones no lineales y su capacidad de proporcionar interpretabilidad en los resultados.\n",
    "\n",
    "3. **Bosque Aleatorio (Random Forest)**:\n",
    "   - Este modelo es un conjunto de múltiples árboles de decisión entrenados en diferentes subconjuntos de los datos. Combina las predicciones de múltiples árboles para mejorar la precisión y reducir el sobreajuste.\n",
    "   - **Ventajas**: Generalmente proporciona buenos resultados en una amplia variedad de tareas de clasificación, y es menos propenso al sobreajuste.\n",
    "\n",
    "#### Implementación de los pipelines:\n",
    "\n",
    "En PySpark, un **pipeline** es una secuencia de etapas, donde cada etapa es un transformador o un estimador. Esto permite encapsular tanto el preprocesamiento como el entrenamiento en una estructura clara y organizada.\n",
    "\n",
    "#### Descripción del código:\n",
    "\n",
    "1. **Diccionario de modelos**:\n",
    "   - Creamos un diccionario llamado **models** que contiene los tres algoritmos que queremos entrenar. Cada modelo está configurado con la columna de etiquetas (`labelCol`) y la columna de características (`featuresCol`).\n",
    "   - La ventaja de organizar los modelos en un diccionario es que nos permite iterar fácilmente sobre ellos para su entrenamiento y evaluación posterior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7157f343-3bc2-4271-91f5-30a289d1e160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar varios modelos de clasificación utilizando pipelines\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol=\"Class\", featuresCol=\"features\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"Class\", featuresCol=\"features\"),\n",
    "    \"Random Forest\": RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b2f120-9311-46f1-b676-cdae6af322c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluadores de métricas adicionales\n",
    "\n",
    "En esta sección, se configuran varios **evaluadores** para medir el rendimiento de los modelos de clasificación que hemos creado. Evaluar correctamente el modelo es crucial para comprender qué tan bien se ajusta a los datos y cómo generaliza en los datos de prueba. Aquí se utilizan métricas que son comunes en problemas de clasificación, tales como la precisión, el recall, el F1 score y el área bajo la curva ROC.\n",
    "\n",
    "#### Métricas utilizadas:\n",
    "\n",
    "1. **Precisión ponderada (Weighted Precision)**:\n",
    "   - La precisión mide la proporción de verdaderos positivos sobre todas las instancias clasificadas como positivas. La versión \"ponderada\" toma en cuenta el desbalance en las clases, lo que es importante en problemas de clasificación multiclase.\n",
    "   - **¿Por qué es importante?**: Nos ayuda a entender cuántas predicciones hechas por el modelo son realmente correctas, considerando el desbalance de clases.\n",
    "\n",
    "2. **Recall ponderado (Weighted Recall)**:\n",
    "   - El recall mide la proporción de verdaderos positivos sobre todas las instancias que son realmente positivas. Al igual que la precisión, la versión \"ponderada\" ajusta este valor teniendo en cuenta el desbalance en las clases.\n",
    "   - **¿Por qué es importante?**: El recall es crucial cuando los falsos negativos son costosos. Un alto recall indica que el modelo captura correctamente la mayoría de las instancias positivas.\n",
    "\n",
    "3. **F1 score**:\n",
    "   - El F1 score es la media armónica entre la precisión y el recall. Proporciona un balance entre ambos, siendo útil cuando es importante encontrar un equilibrio entre precisión y recall.\n",
    "   - **¿Por qué es importante?**: El F1 score es una métrica robusta que equilibra la precisión y el recall, ideal en escenarios donde ambos son igual de importantes.\n",
    "\n",
    "4. **Área bajo la curva ROC (AUC-ROC)**:\n",
    "   - Aunque generalmente se usa para problemas binarios, también se puede adaptar para problemas multiclase. El área bajo la curva ROC (AUC-ROC) mide la capacidad del modelo para distinguir entre las diferentes clases.\n",
    "   - **¿Por qué es importante?**: El AUC-ROC es particularmente útil para evaluar la calidad de las predicciones de probabilidad que genera el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f713b6c-6fe7-41d7-94bd-db416bb29a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluadores para métricas adicionales\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "# AUC-ROC es más relevante para problemas binarios, pero se puede adaptar para problemas multiclase\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Diccionario para almacenar los resultados\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d9b59f9-e2a4-4cc1-ad31-649e8c12c998",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Entrenamiento y Evaluación de Modelos con Pipelines\n",
    "\n",
    "En esta sección, entrenamos y evaluamos varios modelos de clasificación utilizando **pipelines** en PySpark. El pipeline es una herramienta muy poderosa, ya que nos permite definir un flujo de trabajo que combina el ensamblaje de las características (`VectorAssembler`) y el modelo de clasificación seleccionado en una sola estructura. Esto facilita la replicación del flujo para diferentes modelos de manera eficiente.\n",
    "\n",
    "#### Modelos Utilizados:\n",
    "Se han considerado tres modelos de clasificación para este problema de clasificación multiclase:\n",
    "1. **Regresión Logística (Logistic Regression)**\n",
    "2. **Árbol de Decisión (Decision Tree)**\n",
    "3. **Bosque Aleatorio (Random Forest)**\n",
    "\n",
    "Estos modelos se seleccionaron por su rendimiento en problemas de clasificación y su capacidad para capturar diferentes relaciones en los datos:\n",
    "- **Regresión Logística** es ideal para problemas donde las clases son linealmente separables.\n",
    "- **Árbol de Decisión** permite capturar relaciones no lineales y es fácil de interpretar.\n",
    "- **Bosque Aleatorio** mejora el rendimiento mediante la agregación de múltiples árboles de decisión, reduciendo la varianza.\n",
    "\n",
    "#### Flujo de Trabajo del Pipeline:\n",
    "1. **Definir el Pipeline**: Para cada modelo, definimos un pipeline que incluye dos etapas: \n",
    "   - Ensamblaje de características (`VectorAssembler`) que convierte las columnas de entrada en una única columna de características.\n",
    "   - El modelo de clasificación que se está evaluando.\n",
    "\n",
    "2. **Entrenar el Pipeline**: Se entrena el pipeline utilizando el conjunto de datos de entrenamiento. Esto entrena el ensamblador y el modelo en una sola llamada.\n",
    "\n",
    "3. **Hacer Predicciones**: Una vez que el pipeline ha sido entrenado, se realizan predicciones sobre el conjunto de datos de prueba.\n",
    "\n",
    "4. **Evaluar el Modelo**:\n",
    "   - Se utilizan evaluadores de clasificación multiclase para calcular las siguientes métricas:\n",
    "     - **Precisión (Accuracy)**: Proporción de instancias clasificadas correctamente.\n",
    "     - **Precisión ponderada (Precision)**: Proporción de predicciones correctas dentro de todas las predicciones realizadas para una clase.\n",
    "     - **Recall ponderado (Recall)**: Proporción de verdaderos positivos capturados por el modelo.\n",
    "     - **F1 Score**: Promedio armónico entre precisión y recall.\n",
    "     - **Área bajo la curva ROC (AUC)**: Utilizado en clasificación binaria, se muestra solo si el modelo predice dos clases.\n",
    "\n",
    "5. **Resultados**: Los resultados se almacenan en un diccionario para cada modelo, con las métricas de rendimiento clave. Esto permite comparar fácilmente el rendimiento de los distintos modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba341ca-6d21-4cb9-b851-f2d33c473220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Definir el pipeline\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    pipeline = Pipeline(stages=[assembler, model])\n",
    "    \n",
    "    # Entrenar el pipeline\n",
    "    pipeline_model = pipeline.fit(train_data)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = pipeline_model.transform(test_data)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    precision = precision_evaluator.evaluate(predictions)\n",
    "    recall = recall_evaluator.evaluate(predictions)\n",
    "    f1_score = f1_evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Use AUC evaluator only for binary classification\n",
    "    if len(predictions.select(\"rawPrediction\").first()[0]) == 2:\n",
    "        auc = auc_evaluator.evaluate(predictions)\n",
    "    else:\n",
    "        auc = None\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}, AUC: {auc if auc is not None else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a711694a-0163-4907-868a-91243ae9ed43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Selección del mejor modelo basado en F1-Score\n",
    "\n",
    "Después de entrenar y evaluar varios modelos de clasificación utilizando pipelines, el siguiente paso es identificar el modelo que ha tenido el mejor rendimiento según una métrica de evaluación específica. En este caso, utilizamos el **F1-Score** como criterio principal para seleccionar el mejor modelo.\n",
    "\n",
    "#### ¿Por qué F1-Score?\n",
    "\n",
    "El **F1-Score** es una métrica robusta que equilibra dos métricas importantes: la **precisión** (qué porcentaje de nuestras predicciones positivas son correctas) y el **recall** (qué porcentaje de los positivos reales fueron capturados por el modelo). Es especialmente útil cuando el balance entre estas dos métricas es crucial, como en problemas donde tanto los falsos positivos como los falsos negativos son importantes.\n",
    "\n",
    "El F1-Score se utiliza generalmente cuando tenemos un conjunto de datos desbalanceado o cuando tanto los falsos positivos como los falsos negativos deben minimizarse. Por eso, es una excelente métrica para seleccionar el modelo que mejor generaliza a datos no vistos.\n",
    "\n",
    "#### Descripción del proceso:\n",
    "\n",
    "1. **Buscar el mejor modelo**:\n",
    "   - Usamos la función `max()` para iterar a través del diccionario `results` y seleccionar el modelo con el valor más alto de **F1-Score**.\n",
    "   - El **lambda** es una función anónima que nos permite acceder al valor del F1-Score para cada modelo en el diccionario.\n",
    "\n",
    "2. **Imprimir el mejor modelo**:\n",
    "   - Una vez identificado el mejor modelo, se imprime el nombre del modelo junto con su F1-Score, lo que permite identificar cuál de los algoritmos de clasificación fue el más efectivo en este experimento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ad3dfe-3042-4308-9352-8ee6ff522bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar el mejor modelo basado en F1-Score (puedes cambiar esto según tu criterio)\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
    "print(f\"Mejor modelo: {best_model_name} con un F1-Score de {results[best_model_name]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "920cedde-0bd0-41ff-a7cb-ccce56535dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualización y documentación de las métricas del modelo\n",
    "\n",
    "Después de entrenar y evaluar varios modelos de clasificación, es crucial visualizar y comparar las métricas clave para identificar el modelo que ofrece el mejor rendimiento. En esta sección, se crearán gráficas para representar visualmente los resultados de las métricas, y se documentarán en un archivo de texto para futuras referencias.\n",
    "\n",
    "#### Visualización de métricas\n",
    "\n",
    "Utilizamos **matplotlib** para crear gráficos de barras que muestran las siguientes métricas para cada modelo:\n",
    "1. **Precisión** (Accuracy)\n",
    "2. **Precisión ponderada** (Precision)\n",
    "3. **Exhaustividad** (Recall)\n",
    "4. **F1-Score**\n",
    "5. **AUC-ROC**\n",
    "\n",
    "Cada una de estas métricas es esencial para entender el rendimiento del modelo en tareas de clasificación. Además, los gráficos nos ayudan a comparar fácilmente el rendimiento entre los diferentes modelos.\n",
    "\n",
    "#### Explicación de las métricas visualizadas:\n",
    "- **Precisión (Accuracy)**: Proporción de instancias correctamente clasificadas.\n",
    "- **Precisión ponderada (Precision)**: Proporción de predicciones correctas sobre las instancias predichas como positivas, ajustada por el desbalance de clases.\n",
    "- **Recall ponderado (Recall)**: Proporción de instancias positivas correctamente identificadas por el modelo, también ajustada por el desbalance de clases.\n",
    "- **F1-Score**: Promedio armónico entre precisión y recall, útil para evaluar el equilibrio entre ambas métricas.\n",
    "- **AUC-ROC**: Área bajo la curva ROC, mide la capacidad del modelo para distinguir entre las clases en términos de probabilidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8869b951-1d65-447f-b153-c1dad2a5a3ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer las métricas de los resultados y manejar valores None\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[model]['accuracy'] if results[model]['accuracy'] is not None else 0 for model in model_names]\n",
    "precisions = [results[model]['precision'] if results[model]['precision'] is not None else 0 for model in model_names]\n",
    "recalls = [results[model]['recall'] if results[model]['recall'] is not None else 0 for model in model_names]\n",
    "f1_scores = [results[model]['f1_score'] if results[model]['f1_score'] is not None else 0 for model in model_names]\n",
    "aucs = [results[model]['auc'] if results[model]['auc'] is not None else 0 for model in model_names]\n",
    "\n",
    "# Crear una figura con subplots para cada métrica\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "# Graficar la precisión\n",
    "axs[0, 0].bar(model_names, accuracies, color=['blue', 'green', 'red'])\n",
    "axs[0, 0].set_title('Precisión de Modelos de Clasificación')\n",
    "axs[0, 0].set_xlabel('Modelos')\n",
    "axs[0, 0].set_ylabel('Precisión')\n",
    "axs[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Graficar la precisión (Precision)\n",
    "axs[0, 1].bar(model_names, precisions, color=['blue', 'green', 'red'])\n",
    "axs[0, 1].set_title('Precisión (Precision) de Modelos de Clasificación')\n",
    "axs[0, 1].set_xlabel('Modelos')\n",
    "axs[0, 1].set_ylabel('Precisión')\n",
    "axs[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Graficar la exhaustividad (Recall)\n",
    "axs[1, 0].bar(model_names, recalls, color=['blue', 'green', 'red'])\n",
    "axs[1, 0].set_title('Exhaustividad (Recall) de Modelos de Clasificación')\n",
    "axs[1, 0].set_xlabel('Modelos')\n",
    "axs[1, 0].set_ylabel('Exhaustividad')\n",
    "axs[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# Graficar el F1-Score\n",
    "axs[1, 1].bar(model_names, f1_scores, color=['blue', 'green', 'red'])\n",
    "axs[1, 1].set_title('F1-Score de Modelos de Clasificación')\n",
    "axs[1, 1].set_xlabel('Modelos')\n",
    "axs[1, 1].set_ylabel('F1-Score')\n",
    "axs[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Graficar el AUC-ROC\n",
    "axs[2, 0].bar(model_names, aucs, color=['blue', 'green', 'red'])\n",
    "axs[2, 0].set_title('AUC-ROC de Modelos de Clasificación')\n",
    "axs[2, 0].set_xlabel('Modelos')\n",
    "axs[2, 0].set_ylabel('AUC-ROC')\n",
    "axs[2, 0].set_ylim(0, 1)\n",
    "\n",
    "# Ajustar el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar la gráfica en un archivo\n",
    "plt.savefig('model_comparison_metrics.png')\n",
    "\n",
    "# Mostrar la gráfica (esto no funcionará en entornos no interactivos)\n",
    "plt.show()\n",
    "\n",
    "# Documentar los resultados y las evidencias\n",
    "with open('model_results.txt', 'w') as f:\n",
    "    for model_name, metrics in results.items():\n",
    "        accuracy = metrics['accuracy'] if metrics['accuracy'] is not None else 0\n",
    "        precision = metrics['precision'] if metrics['precision'] is not None else 0\n",
    "        recall = metrics['recall'] if metrics['recall'] is not None else 0\n",
    "        f1_score = metrics['f1_score'] if metrics['f1_score'] is not None else 0\n",
    "        auc = metrics['auc'] if metrics['auc'] is not None else 0\n",
    "        f.write(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}, AUC: {auc:.4f}\\n\")\n",
    "    f.write(f\"\\nMejor modelo: {best_model_name} con un F1-Score de {results[best_model_name]['f1_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cf95378-1ce7-45da-a4c3-1ca881b40860",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Entrenamiento del modelo Random Forest y predicción sobre nuevas muestras\n",
    "\n",
    "En esta sección, entrenamos un modelo de **Random Forest** para clasificar las muestras de vino basándonos en sus características químicas. El modelo se entrena utilizando **pipelines** para un flujo más eficiente y reproducible. Además, probamos el modelo con nuevas muestras de vino y mostramos las predicciones junto con las probabilidades asociadas a cada clase.\n",
    "\n",
    "#### 1. Selección de características y división de los datos\n",
    "Primero, seleccionamos las columnas correspondientes a las características (todas excepto la columna 'Class', que es la variable objetivo). Luego, dividimos los datos en conjuntos de entrenamiento (80%) y prueba (20%) de forma aleatoria para evaluar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7964e4-a834-4ff6-a0aa-449edd303102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar las características y la etiqueta\n",
    "feature_columns = df_spark.columns[1:]  # Excluir la columna 'Class'\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca8a7c3-1c05-4823-8988-e519043fc843",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Entrenamiento del modelo Random Forest utilizando pipelines\n",
    "Utilizamos el algoritmo Random Forest, que es adecuado para tareas de clasificación con datos complejos debido a su capacidad para combinar múltiples árboles de decisión y mejorar la generalización. El pipeline combina la transformación de las características (VectorAssembler) y el modelo de clasificación en un solo flujo de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953dbcff-86cc-4303-bd0b-51a1cb020c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo de Random Forest utilizando pipelines\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", probabilityCol=\"probability\")\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "pipeline_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd12ddf-98e0-429b-95a5-8a6a15549d76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Evaluación del modelo\n",
    "Una vez entrenado el modelo, lo evaluamos en el conjunto de datos de prueba utilizando la métrica de precisión (accuracy) para medir qué tan bien el modelo ha clasificado las muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2771a8-be0d-4e22-98e4-9a60064d4beb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "predictions_test = pipeline_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac491046-b6eb-40a8-aada-6382d866ae94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Predicción sobre nuevas muestras\n",
    "A continuación, realizamos predicciones sobre dos nuevas muestras de vino para determinar su clase. Las nuevas muestras son ingresadas manualmente en forma de lista y convertidas a un DataFrame de Spark. Luego, utilizamos el pipeline entrenado para hacer las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c456129-f04e-47f2-8147-c380770e9b20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nuevas muestras de vino\n",
    "new_samples = [\n",
    "    [13.72, 1.43, 2.5, 16.7, 108, 3.4, 3.67, 0.19, 2.04, 6.8, 0.89, 2.87, 1285],\n",
    "    [12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520]\n",
    "]\n",
    "\n",
    "# Crear un DataFrame de Spark para las nuevas muestras\n",
    "new_samples_df = spark.createDataFrame(new_samples, schema=feature_columns)\n",
    "\n",
    "# Hacer predicciones sobre las nuevas muestras directamente con el pipeline\n",
    "predictions_new_samples = pipeline_model.transform(new_samples_df)\n",
    "\n",
    "# Mostrar los resultados de las predicciones con las probabilidades\n",
    "predictions_new_samples.select(\"features\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1468c5d-1931-404b-a66c-f2f5494a257f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aplicación de PCA para reducir la dimensionalidad\n",
    "\n",
    "En esta sección, se implementa el uso de **Análisis de Componentes Principales (PCA)** para reducir la dimensionalidad de los datos. Esta técnica es útil cuando las características originales tienen alta correlación, lo que puede generar redundancia y afectar el rendimiento del modelo. PCA transforma las características originales en un conjunto de nuevas características (componentes principales) que capturan la mayor cantidad de variación posible en los datos con un número reducido de dimensiones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e448424-6bb1-4546-8231-b939ae91645b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertir DataFrame de pandas a Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Seleccionar las características y la etiqueta\n",
    "feature_columns = df_spark.columns[1:]  # Excluir la columna 'Class'\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Aplicar PCA para reducir la dimensionalidad\n",
    "pca = PCA(k=5, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest utilizando pipelines\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"pcaFeatures\")\n",
    "pipeline = Pipeline(stages=[assembler, pca, rf])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest with PCA Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Nuevas muestras de vino\n",
    "new_samples = [\n",
    "    [13.72, 1.43, 2.5, 16.7, 108, 3.4, 3.67, 0.19, 2.04, 6.8, 0.89, 2.87, 1285],\n",
    "    [12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520]\n",
    "]\n",
    "\n",
    "# Crear un DataFrame de Spark para las nuevas muestras\n",
    "new_samples_df = spark.createDataFrame(new_samples, schema=feature_columns)\n",
    "\n",
    "# Hacer predicciones sobre las nuevas muestras directamente con el pipeline\n",
    "predictions_new_samples = pipeline_model.transform(new_samples_df)\n",
    "\n",
    "# Mostrar los resultados de las predicciones con las probabilidades\n",
    "predictions_new_samples.select(\"pcaFeatures\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb0d9abb-0952-4c08-b731-2904184cb049",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aplicación de PCA y Cross-Validation con ajuste de hiperparámetros\n",
    "\n",
    "En esta sección, no solo aplicamos **Análisis de Componentes Principales (PCA)** para reducir la dimensionalidad, sino que también utilizamos un proceso de **validación cruzada (Cross-Validation)** para ajustar los hiperparámetros del modelo de **Random Forest**. Este enfoque nos permite encontrar la mejor combinación de parámetros para maximizar la precisión del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f515bbb9-6d36-4063-8448-27bcd8a70766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convertir DataFrame de pandas a Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Seleccionar las características y la etiqueta\n",
    "feature_columns = df_spark.columns[1:]  # Excluir la columna 'Class'\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Aplicar PCA para reducir la dimensionalidad\n",
    "pca = PCA(k=5, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest utilizando pipelines\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"pcaFeatures\")\n",
    "pipeline = Pipeline(stages=[assembler, pca, rf])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Crear una cuadrícula de hiperparámetros para Random Forest\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [10, 20, 30])\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "             .build())\n",
    "\n",
    "# Crear el CrossValidator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Entrenar el modelo utilizando CrossValidator\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest with PCA and Cross-Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Nuevas muestras de vino\n",
    "new_samples = [\n",
    "    [13.72, 1.43, 2.5, 16.7, 108, 3.4, 3.67, 0.19, 2.04, 6.8, 0.89, 2.87, 1285],\n",
    "    [12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520]\n",
    "]\n",
    "\n",
    "# Crear un DataFrame de Spark para las nuevas muestras\n",
    "new_samples_df = spark.createDataFrame(new_samples, schema=feature_columns)\n",
    "\n",
    "# Hacer predicciones sobre las nuevas muestras directamente con el pipeline\n",
    "predictions_new_samples = cv_model.transform(new_samples_df)\n",
    "\n",
    "# Mostrar los resultados de las predicciones con las probabilidades\n",
    "predictions_new_samples.select(\"pcaFeatures\", \"prediction\", \"probability\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c1910b-1616-4d12-b814-06be027f0175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detener la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wine-classification (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
